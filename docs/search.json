[
  {
    "objectID": "assignments/06-rnaseq-snp.html",
    "href": "assignments/06-rnaseq-snp.html",
    "title": "Alignment Data",
    "section": "",
    "text": "Assignment\n\n\n\nCreate and inspect and alignment files."
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#download-alignment-data-and-visualise",
    "href": "assignments/06-rnaseq-snp.html#download-alignment-data-and-visualise",
    "title": "Alignment Data",
    "section": "Download alignment data and visualise",
    "text": "Download alignment data and visualise\ncd ../data\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/scrubbed/120321-cvBS/19F_R1_val_1_bismark_bt2_pe.deduplicated.sorted.bam\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/scrubbed/120321-cvBS/19F_R1_val_1_bismark_bt2_pe.deduplicated.sorted.bam.bai\ncd ../data\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/data/Cvirg-genome/GCF_002022765.2_C_virginica-3.0_genomic.fa\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/data/Cvirg-genome/GCF_002022765.2_C_virginica-3.0_genomic.fa.fai"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#step-1-quality-control-with-fastqc",
    "href": "assignments/06-rnaseq-snp.html#step-1-quality-control-with-fastqc",
    "title": "Alignment Data",
    "section": "Step 1: Quality control with FastQC",
    "text": "Step 1: Quality control with FastQC\nDownload and install FastQC from https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ and run it on your sample FASTQ file:\nfastqc sample.fastq\nExamine the generated report to assess the quality of your reads."
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#step-2-read-trimming-with-trimmomatic",
    "href": "assignments/06-rnaseq-snp.html#step-2-read-trimming-with-trimmomatic",
    "title": "Alignment Data",
    "section": "Step 2: Read trimming with Trimmomatic",
    "text": "Step 2: Read trimming with Trimmomatic\nDownload and install Trimmomatic from http://www.usadellab.org/cms/?page=trimmomatic. Trim low-quality bases and adapter sequences from your sample file:\njava -jar trimmomatic-0.39.jar SE -phred33 sample.fastq sample_trimmed.fastq ILLUMINACLIP:TruSeq3-SE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#step-3-align-reads-with-hisat2",
    "href": "assignments/06-rnaseq-snp.html#step-3-align-reads-with-hisat2",
    "title": "Alignment Data",
    "section": "Step 3: Align reads with HISAT2",
    "text": "Step 3: Align reads with HISAT2\nBuild an index for your reference genome (you only need to do this once):\nhisat2-build reference_genome.fa reference_genome_index\nAlign your trimmed reads to the reference genome:\nhisat2 -x reference_genome_index -U sample_trimmed.fastq -S sample.sam"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#step-4-process-alignments-with-samtools",
    "href": "assignments/06-rnaseq-snp.html#step-4-process-alignments-with-samtools",
    "title": "Alignment Data",
    "section": "Step 4: Process alignments with SAMtools",
    "text": "Step 4: Process alignments with SAMtools\nDownload and install SAMtools from http://www.htslib.org/. Convert the SAM file to BAM format, sort, and index the alignments:\nsamtools view -bS sample.sam > sample.bam\nsamtools sort sample.bam -o sample_sorted.bam\nsamtools index sample_sorted.bam"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#step-5-snp-calling-with-gatk",
    "href": "assignments/06-rnaseq-snp.html#step-5-snp-calling-with-gatk",
    "title": "Alignment Data",
    "section": "Step 5: SNP calling with GATK",
    "text": "Step 5: SNP calling with GATK\nDownload and install GATK from https://gatk.broadinstitute.org/hc/en-us. Create a reference genome index and call SNPs using HaplotypeCaller:\ngatk CreateSequenceDictionary -R reference_genome.fa\nsamtools faidx reference_genome.fa\ngatk --java-options \"-Xmx4G\" HaplotypeCaller -R reference_genome.fa -I sample_sorted.bam -O sample_raw_variants.vcf"
  },
  {
    "objectID": "assignments/01-blast.html",
    "href": "assignments/01-blast.html",
    "title": "NCBI Blast",
    "section": "",
    "text": "For the first task you will take an unknown multi-fasta file and annotate it using blast. You are welcome to do this in terminal, Rstudio, or jupyter. My recommendation, and how I will demonstrate is using Rmarkdown. Once you have have your project structured, we will download software, databases, a fasta file and run the code.\nIn your code directory create a file.\n01-blast.Rmd"
  },
  {
    "objectID": "assignments/01-blast.html#joining-blast-table-with-annotation-table",
    "href": "assignments/01-blast.html#joining-blast-table-with-annotation-table",
    "title": "NCBI Blast",
    "section": "Joining blast table with annotation table",
    "text": "Joining blast table with annotation table\nAt this point we have a blast output table and annotation table both with a Uniprot accession number. Thus we can join the two tables and be able to get more functional information about thet genes.\n```{bash}\nhead -2 ../output/Ab_4-uniprot_blastx.tab\nwc -l ../output/Ab_4-uniprot_blastx.tab\n```\n```{bash}\ntr '|' '\\t' < ../output/Ab_4-uniprot_blastx.tab | head -2\n```\n```{bash}\ntr '|' '\\t' < ../output/Ab_4-uniprot_blastx.tab \\\n> ../output/Ab_4-uniprot_blastx_sep.tab\n```\n```{bash}\nhead -2 ../data/uniprot_table_r2023_01.tab\nwc -l ../data/uniprot_table_r2023_01.tab\n```\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(\"kableExtra\")\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\nbltabl <- read.csv(\"../output/Ab_4-uniprot_blastx_sep.tab\", sep = '\\t', header = FALSE)\n\n\nspgo <- read.csv(\"../data/uniprot_table_r2023_01.tab\", sep = '\\t', header = TRUE)\n\n\nstr(spgo)\n\n'data.frame':   569213 obs. of  17 variables:\n $ Entry                             : chr  \"A0A023I7E1\" \"A0A024B7W1\" \"A0A024SC78\" \"A0A024SH76\" ...\n $ Reviewed                          : chr  \"reviewed\" \"reviewed\" \"reviewed\" \"reviewed\" ...\n $ Entry.Name                        : chr  \"ENG1_RHIMI\" \"POLG_ZIKVF\" \"CUTI1_HYPJR\" \"GUX2_HYPJR\" ...\n $ Protein.names                     : chr  \"Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3-beta-glucanase 1) (EC 3.2.1.39) (Laminarinase) (RmLam81A)\" \"Genome polyprotein [Cleaved into: Capsid protein C (Capsid protein) (Core protein); Protein prM (Precursor memb\"| __truncated__ \"Cutinase (EC 3.1.1.74)\" \"Exoglucanase 2 (EC 3.2.1.91) (1,4-beta-cellobiohydrolase) (Cellobiohydrolase 6A) (Cel6A) (Exocellobiohydrolase \"| __truncated__ ...\n $ Gene.Names                        : chr  \"ENG1 LAM81A\" \"\" \"M419DRAFT_76732\" \"cbh2 M419DRAFT_122470\" ...\n $ Organism                          : chr  \"Rhizomucor miehei\" \"Zika virus (isolate ZIKV/Human/French Polynesia/10087PF/2013) (ZIKV)\" \"Hypocrea jecorina (strain ATCC 56765 / BCRC 32924 / NRRL 11460 / Rut C-30) (Trichoderma reesei)\" \"Hypocrea jecorina (strain ATCC 56765 / BCRC 32924 / NRRL 11460 / Rut C-30) (Trichoderma reesei)\" ...\n $ Length                            : int  796 3423 248 471 478 693 333 742 2442 455 ...\n $ Gene.Ontology..molecular.function.: chr  \"glucan endo-1,3-beta-D-glucosidase activity [GO:0042973]; glucan endo-1,3-beta-glucanase activity, C-3 substitu\"| __truncated__ \"4 iron, 4 sulfur cluster binding [GO:0051539]; ATP binding [GO:0005524]; ATP hydrolysis activity [GO:0016887]; \"| __truncated__ \"cutinase activity [GO:0050525]\" \"cellulose 1,4-beta-cellobiosidase activity [GO:0016162]; cellulose binding [GO:0030248]\" ...\n $ Gene.Ontology..GO.                : chr  \"extracellular region [GO:0005576]; glucan endo-1,3-beta-D-glucosidase activity [GO:0042973]; glucan endo-1,3-be\"| __truncated__ \"extracellular region [GO:0005576]; host cell endoplasmic reticulum membrane [GO:0044167]; host cell nucleus [GO\"| __truncated__ \"extracellular region [GO:0005576]; cutinase activity [GO:0050525]\" \"extracellular region [GO:0005576]; cellulose 1,4-beta-cellobiosidase activity [GO:0016162]; cellulose binding [\"| __truncated__ ...\n $ Gene.Ontology..biological.process.: chr  \"cell wall organization [GO:0071555]; polysaccharide catabolic process [GO:0000272]\" \"clathrin-dependent endocytosis of virus by host cell [GO:0075512]; fusion of virus membrane with host endosome \"| __truncated__ \"\" \"cellulose catabolic process [GO:0030245]\" ...\n $ Gene.Ontology..cellular.component.: chr  \"extracellular region [GO:0005576]\" \"extracellular region [GO:0005576]; host cell endoplasmic reticulum membrane [GO:0044167]; host cell nucleus [GO\"| __truncated__ \"extracellular region [GO:0005576]\" \"extracellular region [GO:0005576]\" ...\n $ Gene.Ontology.IDs                 : chr  \"GO:0000272; GO:0005576; GO:0042973; GO:0052861; GO:0052862; GO:0071555\" \"GO:0003724; GO:0003725; GO:0003968; GO:0004252; GO:0004482; GO:0004483; GO:0005198; GO:0005524; GO:0005525; GO:\"| __truncated__ \"GO:0005576; GO:0050525\" \"GO:0005576; GO:0016162; GO:0030245; GO:0030248\" ...\n $ Interacts.with                    : chr  \"\" \"\" \"\" \"\" ...\n $ EC.number                         : chr  \"3.2.1.39\" \"2.1.1.56; 2.1.1.57; 2.7.7.48; 3.4.21.91; 3.6.1.15; 3.6.4.13\" \"3.1.1.74\" \"3.2.1.91\" ...\n $ Reactome                          : chr  \"\" \"\" \"\" \"\" ...\n $ UniPathway                        : chr  \"\" \"\" \"\" \"\" ...\n $ InterPro                          : chr  \"IPR005200;IPR040720;IPR040451;\" \"IPR011492;IPR043502;IPR000069;IPR038302;IPR013755;IPR001122;IPR037172;IPR027287;IPR026470;IPR038345;IPR001157;I\"| __truncated__ \"IPR029058;IPR000675;IPR043580;IPR043579;IPR011150;\" \"IPR016288;IPR036434;IPR035971;IPR000254;IPR001524;\" ...\n\n\n\nkbl(\nhead(\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %>%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %>% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n)\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    V1 \n    V3 \n    V13 \n    Protein.names \n    Organism \n    Gene.Ontology..biological.process. \n    Gene.Ontology.IDs \n  \n \n\n  \n    Ab_contig_3 \n    O42248 \n    0 \n    Guanine nucleotide-binding protein subunit beta-2-like 1 (Receptor of activated protein kinase C) (RACK) \n    Danio rerio (Zebrafish) (Brachydanio rerio) \n    angiogenesis [GO:0001525]; convergent extension involved in gastrulation [GO:0060027]; negative regulation of Wnt signaling pathway [GO:0030178]; positive regulation of gastrulation [GO:2000543]; positive regulation of protein phosphorylation [GO:0001934]; regulation of cell division [GO:0051302]; regulation of establishment of cell polarity [GO:2000114]; regulation of protein localization [GO:0032880]; rescue of stalled ribosome [GO:0072344] \n    GO:0001525; GO:0001934; GO:0005080; GO:0005634; GO:0005737; GO:0005829; GO:0005840; GO:0030178; GO:0032880; GO:0043022; GO:0045182; GO:0051302; GO:0060027; GO:0072344; GO:1990904; GO:2000114; GO:2000543 \n  \n  \n    Ab_contig_5 \n    Q08013 \n    0 \n    Translocon-associated protein subunit gamma (TRAP-gamma) (Signal sequence receptor subunit gamma) (SSR-gamma) \n    Rattus norvegicus (Rat) \n    SRP-dependent cotranslational protein targeting to membrane [GO:0006614] \n    GO:0005783; GO:0005784; GO:0006614 \n  \n  \n    Ab_contig_6 \n    P12234 \n    0 \n    Phosphate carrier protein, mitochondrial (Phosphate transport protein) (PTP) (Solute carrier family 25 member 3) \n    Bos taurus (Bovine) \n    mitochondrial phosphate ion transmembrane transport [GO:1990547]; phosphate ion transmembrane transport [GO:0035435] \n    GO:0005315; GO:0005739; GO:0005743; GO:0015293; GO:0035435; GO:0044877; GO:1990547 \n  \n  \n    Ab_contig_9 \n    Q41629 \n    0 \n    ADP,ATP carrier protein 1, mitochondrial (ADP/ATP translocase 1) (Adenine nucleotide translocator 1) (ANT 1) \n    Triticum aestivum (Wheat) \n    mitochondrial ADP transmembrane transport [GO:0140021]; mitochondrial ATP transmembrane transport [GO:1990544] \n    GO:0005471; GO:0005743; GO:0140021; GO:1990544 \n  \n  \n    Ab_contig_13 \n    Q32NG4 \n    0 \n    Glutamine amidotransferase-like class 1 domain-containing protein 1 (Parkinson disease 7 domain-containing protein 1) \n    Xenopus laevis (African clawed frog) \n     \n    GO:0005576 \n  \n  \n    Ab_contig_23 \n    Q9GNE2 \n    0 \n    60S ribosomal protein L23 (AeRpL17A) (L17A) \n    Aedes aegypti (Yellowfever mosquito) (Culex aegypti) \n    translation [GO:0006412] \n    GO:0003735; GO:0005840; GO:0006412; GO:1990904 \n  \n\n\n\n\n\n\nleft_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %>%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %>% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\")) %>%\n  write_delim(\"../output/blast_annot_go.tab\", delim = '\\t')"
  },
  {
    "objectID": "assignments/00-bash.html",
    "href": "assignments/00-bash.html",
    "title": "bash",
    "section": "",
    "text": "Warning\n\n\n\nFor this self directed tutorial you will need to download data-shell.zip and navigate to using a terminal. This could be “Terminal” within Rstudio, or a stand alone application.\nThe part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called “folders”), which hold files or other directories.\nSeveral commands are frequently used to create, inspect, rename, and delete files and directories. To start exploring them, let’s open a shell window:\nThe dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may show something more elaborate.\nType the command whoami, then press the Enter key (sometimes marked Return) to send the command to the shell. The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are:\nMore specifically, when we type whoami the shell:\nNext, let’s find out where we are by running a command called pwd (which stands for “print working directory”). At any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/jovyan\nTo understand what a “home directory” is, let’s have a look at how the file system as a whole is organized. At the top is the root directory that holds everything else. We refer to it using a slash character / on its own; this is the leading slash in /home/jovyan."
  },
  {
    "objectID": "assignments/00-bash.html#ls",
    "href": "assignments/00-bash.html#ls",
    "title": "bash",
    "section": "ls",
    "text": "ls\nLet’s see what’s in this directory by running ls, which stands for “listing”:\nls\ncreatures  molecules           pizza.cfg\ndata       north-pacific-gyre  solar.pdf\nDesktop    notes.txt           writing\n\nls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. We can make its output more comprehensible by using the flag -F, which tells ls to add a trailing / to the names of directories:\nls -F\ncreatures/  molecules/           pizza.cfg\ndata/       north-pacific-gyre/  solar.pdf\nDesktop/    notes.txt            writing/\nHere, we can see that data-shell contains seven sub-directories. The names that don’t have trailing slashes, like notes.txt, pizza.cfg, and solar.pdf, are plain old files. And note that there is a space between ls and -F: without it, the shell thinks we’re trying to run a command called ls-F, which doesn’t exist."
  },
  {
    "objectID": "assignments/00-bash.html#relative-path",
    "href": "assignments/00-bash.html#relative-path",
    "title": "bash",
    "section": "relative path",
    "text": "relative path\nNow let’s take a look at what’s in data-shell directory by running ls -F data, i.e., the command ls with the arguments -F and data. The second argument — the one without a leading dash — tells ls that we want a listing of something other than our current working directory:\n ls -F data\namino-acids.txt  animal-counts/  animals.txt  elements/  morse.txt  pdb/  planets.txt  salmon.txt  sunspot.txt\nThe output shows us that there are four text files and two sub-sub-directories. Organizing things hierarchically in this way helps us keep track of our work: it’s possible to put hundreds of files in our home directory, just as it’s possible to pile hundreds of printed papers on our desk, but it’s a self-defeating strategy.\nNotice, by the way that we spelled the directory name data. It doesn’t have a trailing slash: that’s added to directory names by ls when we use the -F flag to help us tell things apart. And it doesn’t begin with a slash because it’s a relative path, i.e., it tells ls how to find something from where we are, rather than from the root of the file system."
  },
  {
    "objectID": "assignments/00-bash.html#absolute-path",
    "href": "assignments/00-bash.html#absolute-path",
    "title": "bash",
    "section": "absolute path",
    "text": "absolute path\nIf we run ls -F /data (with a leading slash) we get a different answer, because /data is an absolute path:\nls -F /data\nNote you will get an “No file” warning here. This is because we this directory does not exist.\nThe leading / tells the computer to follow the path from the root of the filesystem, so it always refers to exactly one directory, no matter where we are when we run the command.\nIf we wanted to use the **absolute path* to list out the contents of this directory we could used\nls -F /home/jovyan/data-shell/data/\nNote this would work no matter what our pwd is."
  },
  {
    "objectID": "assignments/00-bash.html#nelles-pipeline-organizing-files",
    "href": "assignments/00-bash.html#nelles-pipeline-organizing-files",
    "title": "bash",
    "section": "Nelle’s Pipeline: Organizing Files",
    "text": "Nelle’s Pipeline: Organizing Files\nKnowing just this much about files and directories, Nelle is ready to organize the files that the protein assay machine will create. First, she creates a directory called north-pacific-gyre (to remind herself where the data came from). Inside that, she creates a directory called 2012-07-03, which is the date she started processing the samples. She used to use names like conference-paper and revised-results, but she found them hard to understand after a couple of years. (The final straw was when she found herself creating a directory called revised-revised-results-3.)\n\nNelle names her directories “year-month-day”, with leading zeroes for months and days, because the shell displays file and directory names in alphabetical order. If she used month names, December would come before July; if she didn’t use leading zeroes, November (‘11’) would come before July (‘7’).\n\nEach of her physical samples is labelled according to her lab’s convention with a unique ten-character ID, such as “NENE01729A”. This is what she used in her collection log to record the location, time, depth, and other characteristics of the sample, so she decides to use it as part of each data file’s name. Since the assay machine’s output is plain text, she will call her files NENE01729A.txt, NENE01812A.txt, and so on. All 1520 files will go into the same directory.\nIf she is in her home directory, Nelle can see what files she has using the command:\nls north-pacific-gyre/2012-07-03/\nThis is a lot to type, but she can let the shell do most of the work. If she types:\nls nor\nand then presses tab, the shell automatically completes the directory name for her:\nls north-pacific-gyre/\nIf she presses tab again, Bash will add 2012-07-03/ to the command, since it’s the only possible completion. Pressing tab again does nothing, since there are 1520 possibilities; pressing tab twice brings up a list of all the files, and so on. This is called tab completion, and we will see it in many other tools as we go on."
  },
  {
    "objectID": "assignments/00-bash.html#key-points",
    "href": "assignments/00-bash.html#key-points",
    "title": "bash",
    "section": "Key Points",
    "text": "Key Points\n\nThe file system is responsible for managing information on the disk.\nInformation is stored in files, which are stored in directories (folders).\nDirectories can also store other directories, which forms a directory tree.\n/ on its own is the root directory of the whole filesystem.\nA relative path specifies a location starting from the current location.\nAn absolute path specifies a location from the root of the filesystem.\nDirectory names in a path are separated with / on Unix, but \\ on Windows.\n.. means “the directory above the current one”; . on its own means “the current directory”.\nMost files’ names are something.extension. The extension isn’t required, and doesn’t guarantee anything, but is normally used to indicate the type of data in the file.\nMost commands take options (flags) which begin with a -."
  },
  {
    "objectID": "assignments/00-bash.html#word-count",
    "href": "assignments/00-bash.html#word-count",
    "title": "bash",
    "section": "word count",
    "text": "word count\nLet’s go into that directory with cd and run the command wc *.pdb. wc is the “word count” command: it counts the number of lines, words, and characters in files. The * in *.pdb matches zero or more characters, so the shell turns *.pdb into a complete list of .pdb files:\ncd molecules\n$ wc *.pdb\n\n  20  156 1158 cubane.pdb\n  12   84  622 ethane.pdb\n   9   57  422 methane.pdb\n  30  246 1828 octane.pdb\n  21  165 1226 pentane.pdb\n  15  111  825 propane.pdb\n 107  819 6081 total\n\nWildcards\n* is a wildcard. It matches zero or more characters, so *.pdb matches ethane.pdb, propane.pdb, and so on. On the other hand, p*.pdb only matches pentane.pdb and propane.pdb, because the ‘p’ at the front only matches itself.\n? is also a wildcard, but it only matches a single character. This means that p?.pdb matches pi.pdb or p5.pdb, but not propane.pdb. We can use any number of wildcards at a time: for example, p*.p?* matches anything that starts with a ‘p’ and ends with ‘.’, ‘p’, and at least one more character (since the ‘?’ has to match one character, and the final * can match any number of characters). Thus, p*.p?* would match preferred.practice, and even p.pi (since the first * can match no characters at all), but not quality.practice (doesn’t start with ‘p’) or preferred.p (there isn’t at least one character after the ‘.p’).\nWhen the shell sees a wildcard, it expands the wildcard to create a list of matching filenames before running the command that was asked for. This means that commands like wc and ls never see the wildcard characters, just what those wildcards matched. This is another example of orthogonal design.\nIf we run wc -l instead of just wc, the output shows only the number of lines per file:\nwc -l *.pdb\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total\nWe can also use -w to get only the number of words, or -c to get only the number of characters."
  },
  {
    "objectID": "assignments/00-bash.html#redirect",
    "href": "assignments/00-bash.html#redirect",
    "title": "bash",
    "section": "redirect",
    "text": "redirect\nWhich of these files is shortest? It’s an easy question to answer when there are only six files, but what if there were 6000? Our first step toward a solution is to run the command:\nwc -l *.pdb > lengths\nThe > tells the shell to redirect the command’s output to a file instead of printing it to the screen. The shell will create the file if it doesn’t exist, or overwrite the contents of that file if it does. (This is why there is no screen output: everything that wc would have printed has gone into the file lengths instead.) ls lengths confirms that the file exists:\nls lengths\nlengths"
  },
  {
    "objectID": "assignments/00-bash.html#cat",
    "href": "assignments/00-bash.html#cat",
    "title": "bash",
    "section": "cat",
    "text": "cat\nWe can now send the content of lengths to the screen using cat lengths. cat stands for “concatenate”: it prints the contents of files one after another. There’s only one file in this case, so cat just shows us what it contains:\ncat lengths\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total"
  },
  {
    "objectID": "assignments/00-bash.html#sort",
    "href": "assignments/00-bash.html#sort",
    "title": "bash",
    "section": "sort",
    "text": "sort\nNow let’s use the sort command to sort its contents. We will also use the -n flag to specify that the sort is numerical instead of alphabetical. This does not change the file; instead, it sends the sorted result to the screen:\nsort -n lengths\n  9  methane.pdb\n 12  ethane.pdb\n 15  propane.pdb\n 20  cubane.pdb\n 21  pentane.pdb\n 30  octane.pdb\n107  total"
  },
  {
    "objectID": "assignments/00-bash.html#head",
    "href": "assignments/00-bash.html#head",
    "title": "bash",
    "section": "head",
    "text": "head\nWe can put the sorted list of lines in another temporary file called sorted-lengths by putting > sorted-lengths after the command, just as we used > lengths to put the output of wc into lengths. Once we’ve done that, we can run another command called head to get the first few lines in sorted-lengths:\nsort -n lengths > sorted-lengths\nhead -1 sorted-lengths\n  9  methane.pdb\nUsing the parameter -1 with head tells it that we only want the first line of the file; -20 would get the first 20, and so on. Since sorted-lengths contains the lengths of our files ordered from least to greatest, the output of head must be the file with the fewest lines."
  },
  {
    "objectID": "assignments/00-bash.html#pipe",
    "href": "assignments/00-bash.html#pipe",
    "title": "bash",
    "section": "pipe",
    "text": "pipe\nIf you think this is confusing, you’re in good company: even once you understand what wc, sort, and head do, all those intermediate files make it hard to follow what’s going on. We can make it easier to understand by running sort and head together:\nsort -n lengths | head -1\n  9  methane.pdb\nThe vertical bar between the two commands is called a pipe. It tells the shell that we want to use the output of the command on the left as the input to the command on the right. The computer might create a temporary file if it needs to, or copy data from one program to the other in memory, or something else entirely; we don’t have to know or care.\nWe can use another pipe to send the output of wc directly to sort, which then sends its output to head:\nwc -l *.pdb | sort -n | head -1\n  9  methane.pdb\n\nHere’s what actually happens behind the scenes when we create a pipe. When a computer runs a program—any program—it creates a process in memory to hold the program’s software and its current state. Every process has an input channel called standard input. (By this point, you may be surprised that the name is so memorable, but don’t worry: most Unix programmers call it “stdin”. Every process also has a default output channel called standard output] (or “stdout”)\n\n\nThe shell is actually just another program. Under normal circumstances, whatever we type on the keyboard is sent to the shell on its standard input, and whatever it produces on standard output is displayed on our screen. When we tell the shell to run a program, it creates a new process and temporarily sends whatever we type on our keyboard to that process’s standard input, and whatever the process sends to standard output to the screen.\nHere’s what happens when we run wc -l *.pdb > lengths. The shell starts by telling the computer to create a new process to run the wc program. Since we’ve provided some filenames as parameters, wc reads from them instead of from standard input. And since we’ve used > to redirect output to a file, the shell connects the process’s standard output to that file.\nIf we run wc -l *.pdb | sort -n instead, the shell creates two processes (one for each process in the pipe) so that wc and sort run simultaneously. The standard output of wc is fed directly to the standard input of sort; since there’s no redirection with >, sort’s output goes to the screen. And if we run wc -l *.pdb | sort -n | head -1, we get three processes with data flowing from the files, through wc to sort, and from sort through head to the screen.\nThis simple idea is why Unix has been so successful. Instead of creating enormous programs that try to do many different things, Unix programmers focus on creating lots of simple tools that each do one job well, and that work well with each other. This programming model is called pipes and filters. We’ve already seen pipes; a filter is a program like wc or sort that transforms a stream of input into a stream of output. Almost all of the standard Unix tools can work this way: unless told to do otherwise, they read from standard input, do something with what they’ve read, and write to standard output.\nThe key is that any program that reads lines of text from standard input and writes lines of text to standard output can be combined with every other program that behaves this way as well. You can and should write your programs this way so that you and other people can put those programs into pipes to multiply their power.\n\n\nRedirecting Input\nAs well as using > to redirect a program’s output, we can use < to redirect its input, i.e., to read from a file instead of from standard input. For example, instead of writing wc ammonia.pdb, we could write wc < ammonia.pdb. In the first case, wc gets a command line parameter telling it what file to open. In the second, wc doesn’t have any command line parameters, so it reads from standard input, but we have told the shell to send the contents of ammonia.pdb to wc’s standard input."
  },
  {
    "objectID": "assignments/05-slidedeck.html",
    "href": "assignments/05-slidedeck.html",
    "title": "Project slidedeck",
    "section": "",
    "text": "Assignment\n\n\n\nTLDR: Create and publish a presentation in Quarto on you research project.\n\n\n\n\n\nStart by going to New File > Quarto Presentation. (Leave render type as default)\nSave file in research repo, in the code directory, with the prefix 05.\nCreate Slides using ## (Heading 2) to title new slide.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use same code and principles that you used for knitting a report in week three. Including the set-up chunk. Chunk options work the same way.\n\n\n\nPublish slides using Rpubs and provide link in readme.\n\n\n\n\nFor this assignment you will want to develop slides that\n\nClearly demonstrate your project goal\nMethods taken\nPreliminary Results\nOutline of next steps for next 4 weeks\n\n\n\n\n\nShow core code\nShow parts of initial data (could be first few lines etc)\nInclude a table\nInclude image\nInclude plot generated from code\nHighlight specific lines of code in code block"
  },
  {
    "objectID": "assignments/03-knit.html",
    "href": "assignments/03-knit.html",
    "title": "Knitting Reports",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\n\n\n\n\nAssignment\n\n\n\nTLDR: Take what you did in week 1 & 2 and improve upon the code such that results have profound meaning, code is explained, and a pretty report is produced.\n\n\n\n\nNext Levelling\nFor the past two weeks you got the job done. Now lets not only learn how you go it done, but improve on the output and spend more time to create an html report. To create an attractive web-accessible report using RMarkdown, here are some steps to consider a strive for.\n\nWrite your RMarkdown content: An RMarkdown file consists of three main parts: YAML header, Markdown text, and R code chunks. Use Markdown for formatting text, and R code chunks to insert code and results.\n\n\n\nyaml\n\n---\ntitle: \"Your Report Title\"\nauthor: \"Your Name\"\ndate: \"April 27, 2023\"\noutput: \n  html_document:\n    theme: readable\n    toc: true\n    toc_float: true\n    number_sections: true\n    code_folding: show\n---\n\nThis example uses the “readable” theme and includes a table of contents, numbered sections, and code folding options.\nOther themes include: “default”, “cerulean”, “journal”, “flatly”, “darkly”, “readable”, “spacelab”, “united”, “cosmo”, “lumen”, “paper”, “sandstone”, “simplex”, “yeti”.\nCode highlights argument must be one of default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, textmate, arrow, or rstudio or a file with extension .theme.\n\nMarkdown text: Use Markdown syntax for text formatting (headings, lists, links, etc.).\n\n## Introduction\n\nThis is a sample report created using RMarkdown. You can add *italic*, **bold**, or [links](https://example.com).\n\n- Bullet point 1\n- Bullet point 2\n\nR code chunks: Insert R code and its output in your report using R code chunks.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n\n```{r example-plot}\nlibrary(ggplot2)\ndata(mtcars)\nggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + theme_minimal()\n```\n\nCustomize the appearance: Use CSS, HTML widgets, or additional R packages to further enhance the visual appeal of your report.\n\n\nCustom CSS: Create a separate CSS file and link it in the YAML header.\nHTML widgets: Use packages like leaflet for maps, DT for tables, or plotly for interactive plots.\nAdditional R packages: Use packages like kableExtra for formatting tables, flexdashboard for dashboard layouts, or formattable for conditional formatting.\n\n\nRender the report: In RStudio, click the “Knit” button to generate the HTML output.\nShare the report: Host the generated HTML file on a web server, or use services like GitHub Pages or RStudio Connect to share your report with others.\n\nBy combining RMarkdown with the right formatting, customizations, and packages, you can create visually appealing, web-accessible reports that effectively communicate your insights and analyses.\n\n\nPractical Aspects\nI would recommend copying prior Rmd files and renaming them with the prefix 03.1, 03.2, etc depending on how many separate pages make sense for you.\nThe specific things I will be looking for this week include\n1. Addressing prior comments\n2. Explaining what code is doing\n3. Making output more easily understandable\n4. Adding unique visual features\n5. Generation of html report.\n\n\n\n\n\n\nNote\n\n\n\nYou would want to only run (eval=TRUE) simple tasks in the knit, which is different than running chunk in Rmd.\n\n\n\n\n\n\n\n\nDanger\n\n\n\nBe careful about committing files, particularly those that are a result of cache. It is likely best to ignore those.\n\n\n\n\nInspiration\n3.1 Blast https://rpubs.com/sr320/1026094\n3.2 RNA-seq https://rpubs.com/sr320/1026190\nwhereas code is\n\nhttps://github.com/course-fish546-2023/steven-coursework/blob/main/assignments/code/3.1-blast.Rmd\nhttps://github.com/course-fish546-2023/steven-coursework/blob/main/assignments/code/3.2-dge.Rmd"
  },
  {
    "objectID": "assignments/02-DGE.html",
    "href": "assignments/02-DGE.html",
    "title": "Differential Gene Expression",
    "section": "",
    "text": "Note\n\n\n\nSee screen recording here for some context though note below code is now revised.\nFor this assignment you will be taking RNA-seq reads off the sequencer, and determining what genes are expressed higher in treatment group A compared to treatments group B. Why would someone want to do this? This can tell you something about the physiological response to a “treatment”, which generally speaking could be anything from environment, disease, developmental stage, tissue, species…\nAs opposed to last week these files will be a little larger and compute effort will increase. It is good to pause here and decide what platform(s) you might want to use for this assignment."
  },
  {
    "objectID": "assignments/02-DGE.html#downloading-reference",
    "href": "assignments/02-DGE.html#downloading-reference",
    "title": "Differential Gene Expression",
    "section": "Downloading reference",
    "text": "Downloading reference\nThis code grabs the Pacific oyster fasta file of genes and does so ignoring the fact that gannet does not have a security certificate to authenticate (--insecure). This is usually not recommended however we know the server.\n```{bash}\ncd ../data\ncurl --insecure -O https://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/rna.fna\n```\n\n\n\n\n\n\nNote\n\n\n\nCreating index can take some time\n\n\nThis code is indexing the file rna.fna while also renaming it as cgigas_roslin_rna.index.\n```{bash}\n/home/shared/kallisto/kallisto \\\nindex -i \\\n../data/cgigas_roslin_rna.index \\\n../data/rna.fna\n```"
  },
  {
    "objectID": "assignments/02-DGE.html#downloading-sequence-reads",
    "href": "assignments/02-DGE.html#downloading-sequence-reads",
    "title": "Differential Gene Expression",
    "section": "Downloading sequence reads",
    "text": "Downloading sequence reads\nSequence reads are on a public server at https://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/nopp/\n\n\n\nSample\nSampleID\n\n\nD-control\nD54\n\n\nD-control\nD55\n\n\nD-control\nD56\n\n\nD-control\nD57\n\n\nD-control\nD58\n\n\nD-control\nD59\n\n\nD-control\nM45\n\n\nD-control\nM46\n\n\nD-control\nM48\n\n\nD-control\nM49\n\n\nD-control\nM89\n\n\nD-control\nM90\n\n\nD-desiccation\nN48\n\n\nD-desiccation\nN49\n\n\nD-desiccation\nN50\n\n\nD-desiccation\nN51\n\n\nD-desiccation\nN52\n\n\nD-desiccation\nN53\n\n\nD-desiccation\nN54\n\n\nD-desiccation\nN55\n\n\nD-desiccation\nN56\n\n\nD-desiccation\nN57\n\n\nD-desiccation\nN58\n\n\nD-desiccation\nN59\n\n\n\nThis code uses recursive feature of wget (see this weeks’ reading) to get all 24 files. Additionally as with curl above we are ignoring the fact there is not security certificate with --no-check-certificate\ncd ../data \nwget --recursive --no-parent --no-directories \\\n--no-check-certificate \\\n--accept '*.fastq.gz' \\\nhttps://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/nopp/\nThe next chunk first creates a subdirectory\nThen performs the following steps:\n\nUses the find utility to search for all files in the ../data/ directory that match the pattern *fastq.gz.\n\nUses the basename command to extract the base filename of each file (i.e., the filename without the directory path), and removes the suffix _L001_R1_001.fastq.gz.\nRuns the kallisto quant command on each input file, with the following options:\n\n\n-i ../data/cgigas_roslin_rna.index: Use the kallisto index file located at ../data/cgigas_roslin_rna.index.\n-o ../output/kallisto_01/{}: Write the output files to a directory called ../output/kallisto_01/ with a subdirectory named after the base filename of the input file (the {} is a placeholder for the base filename).\n-t 40: Use 40 threads for the computation.\n--single -l 100 -s 10: Specify that the input file contains single-end reads (–single), with an average read length of 100 (-l 100) and a standard deviation of 10 (-s 10).\nThe input file to process is specified using the {} placeholder, which is replaced by the base filename from the previous step.\n\n```{bash}\nmkdir ../output/kallisto_01\n\nfind ../data/*fastq.gz \\\n| xargs basename -s _L001_R1_001.fastq.gz | xargs -I{} /home/shared/kallisto/kallisto \\\nquant -i ../data/cgigas_roslin_rna.index \\\n-o ../output/kallisto_01/{} \\\n-t 40 \\\n--single -l 100 -s 10 ../data/{}_L001_R1_001.fastq.gz\n```\nThis command runs the abundance_estimates_to_matrix.pl script from the Trinity RNA-seq assembly software package to create a gene expression matrix from kallisto output files.\nThe specific options and arguments used in the command are as follows:\n\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl: Run the abundance_estimates_to_matrix.pl script from Trinity.\n--est_method kallisto: Specify that the abundance estimates were generated using kallisto.\n--gene_trans_map none: Do not use a gene-to-transcript mapping file.\n--out_prefix ../output/kallisto_01: Use ../output/kallisto_01 as the output directory and prefix for the gene expression matrix file.\n--name_sample_by_basedir: Use the sample directory name (i.e., the final directory in the input file paths) as the sample name in the output matrix.\n\nAnd then there are the kallisto abundance files to use as input for creating the gene expression matrix.\n\n```{bash}\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl \\\n--est_method kallisto \\\n    --gene_trans_map none \\\n    --out_prefix ../output/kallisto_01 \\\n    --name_sample_by_basedir \\\n    ../output/kallisto_01/D54_S145/abundance.tsv \\\n    ../output/kallisto_01/D56_S136/abundance.tsv \\\n    ../output/kallisto_01/D58_S144/abundance.tsv \\\n    ../output/kallisto_01/M45_S140/abundance.tsv \\\n    ../output/kallisto_01/M48_S137/abundance.tsv \\\n    ../output/kallisto_01/M89_S138/abundance.tsv \\\n    ../output/kallisto_01/D55_S146/abundance.tsv \\\n    ../output/kallisto_01/D57_S143/abundance.tsv \\\n    ../output/kallisto_01/D59_S142/abundance.tsv \\\n    ../output/kallisto_01/M46_S141/abundance.tsv \\\n    ../output/kallisto_01/M49_S139/abundance.tsv \\\n    ../output/kallisto_01/M90_S147/abundance.tsv \\\n    ../output/kallisto_01/N48_S194/abundance.tsv \\\n    ../output/kallisto_01/N50_S187/abundance.tsv \\\n    ../output/kallisto_01/N52_S184/abundance.tsv \\\n    ../output/kallisto_01/N54_S193/abundance.tsv \\\n    ../output/kallisto_01/N56_S192/abundance.tsv \\\n    ../output/kallisto_01/N58_S195/abundance.tsv \\\n    ../output/kallisto_01/N49_S185/abundance.tsv \\\n    ../output/kallisto_01/N51_S186/abundance.tsv \\\n    ../output/kallisto_01/N53_S188/abundance.tsv \\\n    ../output/kallisto_01/N55_S190/abundance.tsv \\\n    ../output/kallisto_01/N57_S191/abundance.tsv \\\n    ../output/kallisto_01/N59_S189/abundance.tsv\n```"
  },
  {
    "objectID": "assignments/02-DGE.html#get-degs-based-on-desication",
    "href": "assignments/02-DGE.html#get-degs-based-on-desication",
    "title": "Differential Gene Expression",
    "section": "Get DEGs based on Desication",
    "text": "Get DEGs based on Desication\n```{r}\ndeseq2.colData <- data.frame(condition=factor(c(rep(\"control\", 12), rep(\"desicated\", 12))), \n                             type=factor(rep(\"single-read\", 24)))\nrownames(deseq2.colData) <- colnames(data)\ndeseq2.dds <- DESeqDataSetFromMatrix(countData = countmatrix,\n                                     colData = deseq2.colData, \n                                     design = ~ condition)\n```\n```{r}\ndeseq2.dds <- DESeq(deseq2.dds)\ndeseq2.res <- results(deseq2.dds)\ndeseq2.res <- deseq2.res[order(rownames(deseq2.res)), ]\n```\n```{r}\nhead(deseq2.res)\n```\n```{r}\n# Count number of hits with adjusted p-value less then 0.05\ndim(deseq2.res[!is.na(deseq2.res$padj) & deseq2.res$padj <= 0.05, ])\n```\n```{r}\ntmp <- deseq2.res\n# The main plot\nplot(tmp$baseMean, tmp$log2FoldChange, pch=20, cex=0.45, ylim=c(-3, 3), log=\"x\", col=\"darkgray\",\n     main=\"DEG Dessication  (pval <= 0.05)\",\n     xlab=\"mean of normalized counts\",\n     ylab=\"Log2 Fold Change\")\n# Getting the significant points and plotting them again so they're a different color\ntmp.sig <- deseq2.res[!is.na(deseq2.res$padj) & deseq2.res$padj <= 0.05, ]\npoints(tmp.sig$baseMean, tmp.sig$log2FoldChange, pch=20, cex=0.45, col=\"red\")\n# 2 FC lines\nabline(h=c(-1,1), col=\"blue\")\n```\n```{r}\nwrite.table(tmp.sig, \"../output/DEGlist.tab\", row.names = T)\n```"
  },
  {
    "objectID": "assignments/04-hyak.html",
    "href": "assignments/04-hyak.html",
    "title": "Hyak",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\n\n\n\n\nAssignment\n\n\n\nTLDR: log into hyak, run a job, and transfer the the output using rsync\n\n\n\n\nSetup\n\nGo to https://uwnetid.washington.edu/manage/\nClick the “Computing Services” link on the left\nClick the “Hyak Server” and “Lolo Server” check boxes in the “Inactive Services” section.\nClick the “Subscribe >” button at the bottom of the page.\nRead the notice and click the “Finish” button.\n\nFor two factor authentication, you can either sign up for Duo here and use your smart phone or request a security token here. Duo is much easier.\n\n\nLogging in\n\nOpen your favorite terminal\nType ssh <YourUWNetID>@mox.hyak.uw.edu (replace <YourUWNetID> with your own UW Net ID)\nInput your UWNetID password\nIf you’re signed up for 2-factor authentication via Duo, open your smart phone and approve the connection.\nYou’re logged in to a Login node for Hyak!\n\nExample:\nD-69-91-141-150:~ Sean$ ssh seanb80@mox.hyak.uw.edu\nPassword:\nEnter passcode or select one of the following options:\n\n 1. Duo Push to iOS (XXX-XXX-1239)\n 2. Phone call to iOS (XXX-XXX-1239)\n\nDuo passcode or option [1-2]: 1\nLast login: Thu Jun  8 14:59:10 2017 from d-173-250-161-130.dhcp4.washington.edu\n\n     ** NOTICE **\n     Users need to do all their interactive work, including compiling and\n     building software, on the compute nodes (n####) and NOT on the\n     head/login node (hyak.washington.edu). The login nodes are for\n     interacting with the scheduler and transferring data to and from the\n     system.\n\n     Please visit the Hyak User Wiki for more details\n     http://wiki.hyak.uw.edu\n\n\n[seanb80@mox2 ~]$\n\n\nRunning a job\nOnce logged into mox, navigate to /gscratch/scrubbed/your-username. If the dir is not there you shoud create. For every job you submit I recommend working within a directory. Usually I name these in a data format, but we can just mkdir assign_04.\nTo run a job you need to generate a shell script. Create a shell script in your code directory named 04-job.sh with contents such as.\n#!/bin/bash\n## Job Name\n#SBATCH --job-name=assign4\n## Allocation Definition\n#SBATCH --account=srlab\n#SBATCH --partition=srlab\n## Resources\n## Nodes\n#SBATCH --nodes=1\n## Walltime (days-hours:minutes:seconds format)\n#SBATCH --time=01-08:00:00\n## Memory per node\n#SBATCH --mem=100G\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=sr320@uw.edu\n## Specify the working directory for this job\n#SBATCH --chdir=/gscratch/scrubbed/sr320/assign_04\n\n#run a python script you wrote!\nmodule load intel-python3_2017\n\npython 04-hello.py\n\n# run blastx just to get manual\n/gscratch/srlab/programs/ncbi-blast-2.10.1+/bin/blastx -help\n\n#a few bash commands\npwd \n\nwhoami\n\necho \"yah! I ddi it!!!!!!!!!!\"\n\n#this writes out  tofile\necho \"yah! I ddi it!!!!!!!!!!\" > text.file\nYou will also want to write some python code :)\nCreate a Python Script in the same directory name 04-hello.py with the contents:\nprint(\"Hello, World!\")\nNow we want to move these two files to mox into the assign_04 directory. To to this you will need to type something to the effect of the following in the terminal\nrsync -avz assignments/code/04-* sr320@mox.hyak.uw.edu:/gscratch/scrubbed/sr320/assign_04\nThen on mox, inside the assign_04 directory you will type”\nsbatch 04-job-sh to schedule the job.\nOnce done you should have a couple of new files in the directory.\nYou will want to check them to see if everything worked and then move the output back to your repo…\nrsync -avz sr320@mox.hyak.uw.edu:/gscratch/scrubbed/sr320/assign_04/ ."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lectures/00-before.html",
    "href": "lectures/00-before.html",
    "title": "Course Preparation",
    "section": "",
    "text": "This course is designed for graduate students with core computational competence and an appropriate data set for analyses to be performed during the course. Before class starts (and add codes are distributed) the following tasks need to be completed.\n\nSubmit your github ID using this form.\nEstablish account on Roberts Lab (srlab) hyak account\nRead: Introducing the Shell\nRead: Navigating Files and Directories\nComplete this bash tutorial\nRead: Organize your data and code\nLearn (remember) proper project (repo) structure.\n\n\nFile StructureDataCode\n\n\n\nGood file structure\n\nAll project files in one main folder\nSubfolders (data, code, output)\n\nMain folder is R project\n\nSelf-contained project\nUse relative instead of absolute paths\n\nGood folder & file names\n\nDescriptive but not too long\nNo spaces\nConsistent format\n\n\n\n\n\nRaw data\n\nIn separate folder from cleaned data\nNever change!\nEach file should have metadata\n\n\n\n\n\nScripts with code\n\nRelative file paths to read in and create files\nLots of comments\nOrder: libraries, data, user-created functions, everything else\nGood variable & column names"
  },
  {
    "objectID": "lectures/05-knit-slides.html#creating-a-presentation-in-quarto",
    "href": "lectures/05-knit-slides.html#creating-a-presentation-in-quarto",
    "title": "Knitting up some slides",
    "section": "Creating a Presentation in Quarto",
    "text": "Creating a Presentation in Quarto\nFollow these steps to create a presentation using Quarto:\n\nCreate a new Quarto presentation file: Create a new file with the extension .qmd (e.g., presentation.qmd). This file will contain your presentation content and code chunks.\nAdd YAML metadata: At the top of your .qmd file, include a YAML metadata block to specify the output format and other options. For a presentation, use the following:\n\n---\ntitle: \"Your Presentation Title\"\nformat: beamer\noutput:\n  beamer_presentation:\n    theme: metropolis\n    slide_level: 2\n---\n\nWrite your presentation: Use standard Markdown syntax for formatting your text and headings. To create a new slide, use a level-2 heading (e.g., ## Slide Title). You can include R, Python, or Julia code chunks using the following syntax:\n\n\n```{r}\n# Your R code here\n```\n\n```{python}\n# Your Python code here\n```"
  },
  {
    "objectID": "lectures/06-snps.html",
    "href": "lectures/06-snps.html",
    "title": "Alignment Data and Genetic Variation",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\nReading\nTextbook:\nWorking with Alignment Data pg 355-394\n\n\nObjectives\nBetter understand alignment files, how to visualize them, and how to get SNP information from them.\n\n\nBackground\nOver the past quarter, we have explored various ways of comparing genes or genetic sequences. If you think about it in a broad sense, what we have done is look at letter comparisons, such as with BLAST performing a homology search, and look at genes that are expressed differentially. You could say that this is a good way to consider variance. Many of you in your projects are doing this in different ways, using annotations and/or BLAST. This week, we will consider a different way of looking at the relatedness of genetic information: single nucleotide polymorphisms (SNPs). SNPs are used to describe population variation or phylogenetic relationships. A first step in this is aligning sequence reads to look for variation.\n\n\nSAM/BAM\nThe SAM/BAM format, is commonly used for storing sequencing reads mapped to a reference. A large portion of bioinformatics involves manipulating alignment files and extracting useful information from them. The SAM/BAM formats are space-efficient complex file formats because alignment files are massive. Learning to work with these formats is important because it develops essential skills such as following format specifications, manipulating binary files, and working with APIs that are widely applicable beyond these specific formats.\nSAM (Sequence Alignment/Map) and BAM (Binary Alignment/Map) are two file formats used to store aligned sequencing data. Both formats are designed to store read alignments from high-throughput sequencing experiments to a reference genome. However, they have some key differences:\n1 Representation:\n•   SAM: SAM is a human-readable, plain text format. It consists of a header section with metadata about the reference genome and alignment, followed by a tab-delimited alignment section containing individual aligned reads and their mapping information.\n\n•   BAM: BAM is the binary equivalent of the SAM format, meaning it stores the same information as a SAM file but in a compressed binary form. This compression makes BAM files more storage-efficient and faster to process than SAM files.\n2 File size:\n•   SAM: Due to its plain text nature, SAM files are generally larger than their BAM counterparts.\n\n•   BAM: The binary, compressed format of BAM files results in smaller file sizes compared to SAM files, making them more suitable for large-scale sequencing projects.\n3 Readability:\n•   SAM: Since SAM is a plain text format, it can be easily read and edited using standard text editors or processed with command-line tools.\n\n•   BAM: As a binary format, BAM files are not human-readable and require specialized software tools for reading, editing, and processing.\n4 Processing speed:\n•   SAM: Processing SAM files can be slower because of their larger size and plain text format.\n\n•   BAM: BAM files enable faster processing due to their binary, compressed nature.\nIn summary, SAM is a human-readable, plain text format, while BAM is a compressed binary format. BAM files offer advantages in terms of file size and processing speed, whereas SAM files are more easily read and edited using basic text editing tools. The choice between the two formats depends on the specific requirements of the sequencing project and the tools being used for analysis.\nSAM file example:\n@SQ     SN:ref  LN:45\n@PG     ID:bwa  PN:bwa  CL:bwa mem ref.fa reads.fq\nread1   0       ref     7       60      8M      *       0       0       ATGCACTG  *       NM:i:0  MD:Z:8\nread2   0       ref     7       60      8M      *       0       0       ATGCACTA  *       NM:i:1  MD:Z:7A0\n\nThe first two lines are the header section, containing metadata about the reference genome and the program used to generate the alignment. The following lines represent individual aligned reads in a tab-separated format.\n---\n\n\nSamtools\nSamtools is a software package designed for manipulating and analyzing high-throughput sequencing data, specifically SAM (Sequence Alignment/Map) and BAM (Binary Alignment/Map) files. It is widely used in the field of genomics and bioinformatics for processing and managing next-generation sequencing data.\nHere are the 5 most commonly used subcommands in Samtools:\n\nsamtools view: This subcommand is used to convert between SAM and BAM file formats, filter alignments, and extract specific reads or regions from the input files. It can also output only alignments with specific flags, such as mapped or unmapped reads.\nsamtools sort: This subcommand sorts alignments by their chromosomal coordinates. Sorting is essential for many downstream analyses, as it enables faster access to specific regions of interest and ensures compatibility with other tools.\nsamtools index: This subcommand creates an index file for a sorted BAM file. Indexing allows for efficient random access to specific regions in the BAM file, which is crucial for tools that perform region-based analyses.\nsamtools merge: This subcommand is used to merge multiple sorted BAM files into a single BAM file. This is useful when combining data from different sequencing runs or multiple lanes of a sequencing machine.\nsamtools mpileup: This subcommand generates a summary of the base calls at each position in the reference genome, considering all the aligned reads. The output can be used for variant calling, consensus sequence generation, or assessing sequencing depth at specific genomic positions.\n\nIn addition to these, Samtools offers many other subcommands to perform tasks such as alignment statistics, read group manipulation, and variant calling. The software package is widely used due to its versatility, efficiency, and compatibility with various sequencing platforms and analysis tools.\n\nThis text discusses the pileup format, a plain-text format used to summarize reads’ bases at each chromosome position by stacking aligned reads. The pileup format helps identify variants and determine individual genotypes. The samtools mpileup subcommand creates pileups from BAM files and is the first step in samtools-based variant calling pipelines.\nThe output is saved in a Variant Call Format (VCF) file, which has three parts: a metadata header, a header line with mandatory fields and sample names, and data lines containing information for a variant at a particular position and all individuals’ genotypes.\n``\n\n\nDifferent Means to get SAM/BAM files\n\nWhole Genome Sequencing - Reduced Represenation (DNA; traditional)\nRNA-seq (RNA)\nDNA Methylation specific alignment (DNA)\n\n\n\nSNPs in RNA-seq data\nFinding SNPs (Single Nucleotide Polymorphisms) in RNA-seq data is an essential step in understanding genetic variation and its effects on gene expression. Here is a summary of the process:\n\nQuality control and preprocessing: Start by assessing the quality of your raw RNA-seq reads using tools like FastQC. Trim low-quality bases and adapter sequences using software like Trimmomatic or Cutadapt to ensure accurate downstream analysis.\nAlignment: Align the cleaned RNA-seq reads to a reference genome using a spliced aligner like STAR, HISAT2, or TopHat2. These tools can handle the intron-exon structure of RNA-seq data and provide accurate alignments.\nSort and index alignments: Use SAMtools or Picard to convert the alignment output (SAM) to a binary format (BAM), sort, and index the aligned reads for efficient data processing.\nVariant calling: Employ a variant caller such as GATK’s HaplotypeCaller, SAMtools mpileup, or FreeBayes to identify SNPs and other genetic variants in your RNA-seq data. These tools use statistical models to call SNPs based on the differences observed between aligned reads and the reference genome.\nFiltering: Apply quality filters to remove low-confidence variant calls. Use tools like GATK’s VariantFiltration or BCFtools filter to set quality thresholds based on parameters like depth, quality score, strand bias, and mapping quality.\nAnnotation: Annotate the filtered SNPs using software like ANNOVAR, SnpEff, or VEP to gain insights into their potential functional effects on genes, transcripts, and proteins.\nDownstream analysis: Investigate the potential impact of SNPs on gene expression, alternative splicing, or allele-specific expression using tools like DESeq2, edgeR, or Cufflinks.\n\nRemember that this is only a general summary and specific steps might vary depending on the organism, reference genome, and experimental conditions. Always consult the documentation of the chosen tools and follow best practices for RNA-seq data analysis.\n\n\nShow and Tell\nhttps://rpubs.com/sr320/1035758"
  },
  {
    "objectID": "lectures/01-start-up.html",
    "href": "lectures/01-start-up.html",
    "title": "Getting Started",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\nText Reading\nHow to Learn Bioinformatics 1-18;\nSetting Up and Managing a Bioinformatics Project 21-35;\n\n\n\nObjectives\n\n\nSetting up for Success!\nAs part of this class you will be learning fundamental skills in working with genomic data. In addition you will be carrying out an independent project throughout the quarter. Generally Tuesday will be learning a skillset and Thursday will be working on your independent project.\nEach student will have two GitHub repositories, one where you complete “classwork” and as one devoted to your project. Both need to be in the organization course-fish546-2023.\nThe name of these repos:\npreferredname-classwork and\npreferredname-projectdescriptor\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you have you local repo clone in logical location (eg ~/Documents/GitHub) and that you do not move, nor place in Dropbox or similar syncing directory.\n\n\nBe sure to comply with guidelines\n\nFile StructureDataCode\n\n\n\nGood file structure\n\nAll project files in one main folder\nSubfolders (data, code, output)\n\nMain folder is R project\n\nSelf-contained project\nUse relative instead of absolute paths\n\nGood folder & file names\n\nDescriptive but not too long\nNo spaces\nConsistent format\n\n\n\n\n\nRaw data\n\nIn separate folder from cleaned data\nNever change!\nEach file should have metadata\n\n\n\n\n\nScripts with code\n\nRelative file paths to read in and create files\nLots of comments\nOrder: libraries, data, user-created functions, everything else\nGood variable & column names\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMax GitHub file size is 100MB\n\n\nThere will be times when files are two big to include in repositories (or even you laptop). There will also be times when you have to run code outside of a GitHub repository. You will need to determine a way to effectively document this in your repository.\n\n\nComputers\nTo no surprise you will need to have some type of computer to do your analysis. This might seem trivial, but is not. In some instances you might you multiple machines. Generally speaking there are two primary consideration- RAM and CPUs (ie memory and power). Memory comes into play in running programs need to temporary store information like transcriptome assembly. Power is beneficial from programs that can use mulitple CPUs. For example if hardware has 48 cores, it could run a program faster than one with 4 cores. Note there are a lot of nuances here but it is good to have this vocabulary. Some of the work takes a long time (even on big machines) - meaning hours to weeks, thus hardware access needs consideration.\nSome of the options you have are\n- your personal laptop (borrowed laptop) - duration limited?\n- JupyterHub Instance - UW cloud machine, duration limited,\n- Roberts Lab Raven Rstudio server - cloud machine\n- Hyak Supercomputer - powerful - advanced interface\nFor simply typing (something that is also important) you can do this with almost anything with a keyboard. Note that GitHub will be the platform that allows you to move across machines.\nMost of the Assignments are designed to run on lightweigt hardware, and we might want to try experience different platforms to see what works best for you. It is important to keep in mind that if you using muliptle machines there is the possibility of causing git conflicts.\nOrganization and thought is important, particulary when it comes to this.\n\n\nWorking in the command-line\nHaving already reviewed the prep material and completed the bash tutorial you are now ready to get to coding.\nFor the first task you will take an unknown multi-fasta file and annotate it using blast. You are welcome to do this in terminal, Rstudio, or jupyter. My recommendation, and how I will demonstrate is using Rmarkdown. Once you have have your project structured, we will download software, databases, a fasta file and run the code.\nIn your code directory create a file.\n01-blast.Rmd\n\n\n\n\n\n\nTip\n\n\n\nRmarkdown is a good option as you can use markdown, add pictures and more!"
  },
  {
    "objectID": "lectures/03-rstudio.html#adding-tables",
    "href": "lectures/03-rstudio.html#adding-tables",
    "title": "Rstudio Fundamentals",
    "section": "Adding tables:",
    "text": "Adding tables:\n\nMarkdown syntax: You can create a simple table using pipes | and hyphens -. Here’s an example:\n\n| Column1 | Column2 | Column3 |\n|---------|---------|---------|\n| A       | B       | C       |\n| X       | Y       | Z       |\nThis will create a table with two rows and three columns.\n\nR code: You can create more complex tables using R packages like kable from the knitr package, or gt and flextable. Here’s an example using kable:\n\n```{r}\nlibrary(knitr)\n\ndata <- data.frame(\n  Column1 = c(\"A\", \"X\"),\n  Column2 = c(\"B\", \"Y\"),\n  Column3 = c(\"C\", \"Z\")\n)\n\nkable(data, caption = \"An example table\")\n```\nThis will generate a table with the specified data and caption."
  },
  {
    "objectID": "lectures/03-rstudio.html#adding-images",
    "href": "lectures/03-rstudio.html#adding-images",
    "title": "Rstudio Fundamentals",
    "section": "Adding images:",
    "text": "Adding images:\n\nMarkdown syntax: You can insert an image using the following syntax: ![alt text](path/to/image \"Optional title\"). Here’s an example:\n\n![Example image](path/to/image.jpg \"Optional title\")\nMake sure to replace path/to/image.jpg with the actual file path or URL of the image.\n\nR code: You can also add images using R code, especially if you’re generating images with R plots. Here’s two examples:\n\n```{r}\nplot(cars, main = \"An example plot\", xlab = \"Speed\", ylab = \"Distance\")\n```\n```{r schemat, echo = FALSE, out.width = “70%”, fig.align = “center”}\nknitr::include_graphics(“img/ncbi.png”)\n```\nThe benefit of the this code as opposed to Mardown (above) is that you the ability to change size and align"
  },
  {
    "objectID": "lectures/04-remote.html#reciprocal-blast",
    "href": "lectures/04-remote.html#reciprocal-blast",
    "title": "Remote Computing",
    "section": "Reciprocal BLAST",
    "text": "Reciprocal BLAST\nA reciprocal BLAST (Basic Local Alignment Search Tool) analysis is a technique used to identify homologous genes or proteins between two species by performing a BLAST search in both directions. In other words, it involves searching for similarities between a query sequence from species A against a database of sequences from species B, and then searching for similarities between a query sequence from species B against a database of sequences from species A. This approach helps confirm orthology, which is useful in comparative genomics and evolutionary studies.\nThree insightful visualizations of reciprocal BLAST analysis results could be:\n\nCircos Plot: A Circos plot is a circular layout that can display relationships between genomic sequences. In the context of reciprocal BLAST analysis, a Circos plot can effectively illustrate orthologous relationships between two species by connecting homologous genes or proteins with lines or curves. The strength of the connection (e.g., based on similarity scores or e-values) can be represented by the width or color of the lines, providing a comprehensive overview of the conservation between the two genomes.\nHeatmap: A heatmap is a graphical representation of data where values are represented by colors. In a reciprocal BLAST analysis, a heatmap can be used to visualize the similarity scores or e-values between pairs of orthologous genes or proteins. The rows and columns of the heatmap would represent genes or proteins from species A and species B, respectively, with the color intensity representing the strength of the orthologous relationship. This allows for easy identification of strongly conserved regions or potential functional similarities between the two species.\nScatter Plot with Dot Matrix: A scatter plot with dot matrix can visualize the distribution of reciprocal best hits (RBH) based on similarity scores or e-values. The x-axis would represent the scores or e-values for species A against species B, and the y-axis would represent the scores or e-values for species B against species A. Each dot on the plot would represent a pair of orthologs, with its position determined by the score or e-value in both directions. This visualization allows users to identify potential outliers, observe the overall correlation between the two species, and assess the quality of the reciprocal BLAST analysis results."
  },
  {
    "objectID": "lectures/04-remote.html#saving-my-pat",
    "href": "lectures/04-remote.html#saving-my-pat",
    "title": "Remote Computing",
    "section": "Saving my PAT",
    "text": "Saving my PAT\n\nsee https://happygitwithr.com/https-pat.html?q=password#store-pat\n\ninstall.packages(\"gitcreds\")\ngitcreds::gitcreds_set()"
  },
  {
    "objectID": "lectures/04-remote.html#chat-gpt-in-rstudio",
    "href": "lectures/04-remote.html#chat-gpt-in-rstudio",
    "title": "Remote Computing",
    "section": "Chat GPT in Rstudio",
    "text": "Chat GPT in Rstudio\nVideo of recent lab meeting"
  },
  {
    "objectID": "lectures/02-rna-seq.html#quality-control",
    "href": "lectures/02-rna-seq.html#quality-control",
    "title": "RNAs-seq",
    "section": "Quality Control",
    "text": "Quality Control\nThe first step in analyzing RNA-seq data is to perform quality control checks on the raw fastq files. This step is crucial to ensure that the data is of high quality and can be accurately quantified. One popular tool for quality control is FastQC, which generates various quality metrics such as per-base sequence quality, adapter contamination, and GC content.\nTo perform quality control using FastQC, run the following command:\nfastqc input.fastq\nThis will generate a HTML report that can be viewed in a web browser.\n\nAnother popular quality control program is fastp. Here is a very nice tutorial on using fastp"
  },
  {
    "objectID": "lectures/02-rna-seq.html#marineomics-rna-seq-panel-discussion",
    "href": "lectures/02-rna-seq.html#marineomics-rna-seq-panel-discussion",
    "title": "RNAs-seq",
    "section": "MarineOmics RNA-seq Panel Discussion",
    "text": "MarineOmics RNA-seq Panel Discussion"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Bioinformatics - FISH 546",
    "section": "",
    "text": "Date\nQuestions\nTopic\nAssignment\nProject Status\n\n\n\n\nlast week\n\nPreparing for class\nbash\n\n\n\nmar 27\nweek 01\nGetting Started\nNCBI Blast\nEstablish Repos\n\n\napr 03\nweek 02\nRNA-seq + Ariana Huffmyer\nDifferential Gene Expression\nExplore data, QC, add hashes\n\n\napr 10\nweek 03\nRStudio:knitr/ Fundamentals\nExplain and knit\nID endpoint in Readme. Start workflow.\n\n\napr 17\nweek 04\nRemote Computing\nHyak\nComplete sliced workflow (->endpoint)\n\n\napr 24\nweek 05\nQuatro\nPublishing Slides\nReadme w/ results (links), has plan for 4 weeks, issues resolved\n\n\nmay 01\nweek 06\nGenetic Variation\ntest\n\n\n\nmay 08\nweek 07\nDNA methylation + Matt George\n\n\n\n\nmay 15\nweek 08\nGenomic Ranges\n\n\n\n\nmay 22\nweek 09\nProject focus\n\n\n\n\nmay 29\nweek 10\nPresentations\n\nCompendium"
  },
  {
    "objectID": "questions/week01.html",
    "href": "questions/week01.html",
    "title": "Bioinformatics - FISH 546",
    "section": "",
    "text": "What is your prior experience in this discipline?\nWhat do you hope to get out of this class?\nThis class is strongly rooted in an independent project related to genomic analyses. What specific project do you have in mind? If you do not have any data or preference, data can be provided / aquired. If you do not have a specfic project, what approach would you like to master as part of this class?\nWhat are two things you found most useful from the reading?"
  },
  {
    "objectID": "questions/week06.html",
    "href": "questions/week06.html",
    "title": "Week 06 Questions",
    "section": "",
    "text": "What are SAM/BAM files? What is the difference between to the two?\nsamtools is a popular program for working with alignment data. What are three common tasks that this software is used for?\nWhy might you want to visualize alignment data and what are two program that can be used for this?\nDescribe what VCF file is?"
  },
  {
    "objectID": "questions/week05.html",
    "href": "questions/week05.html",
    "title": "Week 05 Questions",
    "section": "",
    "text": "What is Quarto?\nHow do you make columns using Revealjs in Quarto Presentations?\nHow would you change the appearance of slides using Revealjs in Quarto Presentations?\nWhat has been the biggest constraint working on your own research project in the past week?"
  },
  {
    "objectID": "questions/week04.html",
    "href": "questions/week04.html",
    "title": "Week 04 Questions",
    "section": "",
    "text": "What is tmux and how does this relate to our current way of working on raven?\nWhat is ssh and what would the code be you would type if you were going to ssh into raven?\nWhat has been the most challenging part of your research project? Are you happy with your organization skills? If not what could be improved?\nFor last weeks assignment what did you appreciate the most about knitting documents?"
  },
  {
    "objectID": "questions/week03.html",
    "href": "questions/week03.html",
    "title": "Week 03 Questions",
    "section": "",
    "text": "An R Markdown file is plain text file that contains what 3 important types of content?\nWhat is a chunk and how do you add them? of the many chunk options which one do you think you will use the most and why? How is inline code different than code chunks?\nWhat’s gone wrong with this code? Why are the points not blue?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\nplot\n\n\n\nOf the many things we have done in class the past two weeks, what is one aspect you would like to revisit and spend more time on?"
  },
  {
    "objectID": "questions/week02.html",
    "href": "questions/week02.html",
    "title": "Week 02 Questions",
    "section": "",
    "text": "You can simply copy and paste this markdown text\n## Week 02 Question Set\n\na)  **What do you feel was the most impressive thing you did in class last week was?**\n\nb)  **What is your weekly goal for making progress on your project? What is the next step?**\n\nc)  **There were two readings this week not from the textbook, meant for two different audiences. Which reading did you get the most out of and why? Do you have any questions regarding the Journal of Shellfish Research paper?**\n\nd)  **What is your favorite thing about markdown and why?**\n\n\ne) **What is the differnce between `curl` and `wget`? When would you used one over the other?"
  }
]