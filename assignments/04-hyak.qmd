---
title: "Hyak"
subtitle: "In need mor that just raven"

format:
  html:
    code-fold: false
    code-tools: true
    code-copy: true
    highlight-style: github
    code-overflow: wrap
---

# Screen Recording

[![](/img/zoom.png){fig-align="right"}](xxxxxxxxxxxx)

::: callout-important
## Assignment

TLDR: log into hyak, run a simple command, and transfer the the output using `rsync`
:::

# Setup

1.  Go to <https://uwnetid.washington.edu/manage/>

2.  Click the "Computing Services" link on the left

3.  Click the "Hyak Server" and "Lolo Server" check boxes in the "Inactive Services" section.

4.  Click the "Subscribe \>" button at the bottom of the page.

5.  Read the notice and click the "Finish" button.

For two factor authentication, you can either sign up for Duo [here](https://identity.uw.edu/2fa/) and use your smart phone or request a security token [here](http://www.washington.edu/itconnect/service/authentication/). Duo is much easier.

# Logging in

1.  Open your favorite terminal

2.  Type `ssh <YourUWNetID>@mox.hyak.uw.edu` (replace `<YourUWNetID>` with your own UW Net ID)

3.  Input your UWNetID *password*

4.  If you're signed up for 2-factor authentication via Duo, open your smart phone and approve the connection.

5.  You're logged in to a Login node for Hyak!

Example:

```         
D-69-91-141-150:~ Sean$ ssh seanb80@mox.hyak.uw.edu
Password:
Enter passcode or select one of the following options:

 1. Duo Push to iOS (XXX-XXX-1239)
 2. Phone call to iOS (XXX-XXX-1239)

Duo passcode or option [1-2]: 1
Last login: Thu Jun  8 14:59:10 2017 from d-173-250-161-130.dhcp4.washington.edu

     ** NOTICE **
     Users need to do all their interactive work, including compiling and
     building software, on the compute nodes (n####) and NOT on the
     head/login node (hyak.washington.edu). The login nodes are for
     interacting with the scheduler and transferring data to and from the
     system.

     Please visit the Hyak User Wiki for more details
     http://wiki.hyak.uw.edu


[seanb80@mox2 ~]$
```

# Running a job

In this tutorial, we will guide you through the process of running a simple job on a High-Performance Computing (HPC) cluster using the SLURM workload manager. We will assume that you have a basic understanding of Linux command-line and that you have access to an HPC cluster with SLURM installed.

Step 1: Create a simple code file
First, you need to create a simple code file that you want to run on the HPC cluster. Let's use a Python script for this example. Create a file named **`hello_world.py`** with the following content:

```         
pythonCopy code
```

`print("Hello, World!")`

Step 2: Write a SLURM script
Now, create a SLURM script that will be used to submit your job to the HPC cluster. Create a file named **`slurm_job.sh`** with the following content:

```         
bashCopy code
```

`#!/bin/bash

#SBATCH --job-name=hello_world
#SBATCH --output=hello_world_output.txt
#SBATCH --error=hello_world_error.txt
#SBATCH --time=00:01:00
#SBATCH --partition=your_partition_name
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --mail-type=ALL
#SBATCH --mail-user=your_email@example.com

module load python/3.x.x
python hello_world.py`

Replace **`your_partition_name`** with the name of the partition you want to use on your HPC cluster, and **`your_email@example.com`** with your email address to receive job notifications. The **`module load`** line should be adjusted to match the Python module available on your cluster.

Step 3: Transfer your files to the HPC cluster
Use **`scp`**, **`rsync`**, or any other preferred method to transfer both the **`hello_world.py`** and **`slurm_job.sh`** files to your home directory on the HPC cluster.

Step 4: Submit your job
Once the files are on the HPC cluster, submit your job by running the following command:

```         
Copy code
```

`sbatch slurm_job.sh`

You will receive a job ID after the successful submission of your job. You can monitor the progress of your job using the **`squeue`** command:

```         
Copy code
```

`squeue -u your_username`

Step 5: Retrieve the results
Once the job is completed, you can find the output of your job in the **`hello_world_output.txt`** file. You can view the content of this file using the **`cat`** command:

```         
bashCopy code
```

`cat hello_world_output.txt`

This should display "Hello, World!" in your terminal.

If there were any errors, they would be stored in the **`hello_world_error.txt`** file. You can view this file in the same way:

```         
bashCopy code
```

`cat hello_world_error.txt`

Step 6: Clean up
After you have successfully retrieved your output, you can remove the job files from the HPC cluster if you no longer need them. Use the **`rm`** command to delete the files:

```         
bashCopy code
```

`rm hello_world.py slurm_job.sh hello_world_output.txt hello_world_error.txt`

That's it! You have successfully run a simple job on an HPC cluster using SLURM.
